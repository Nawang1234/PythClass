{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e669561-f70d-4826-8e59-05b5aa22e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "url = 'https://www.meesho.com/accessories-men/pl/3tp'\n",
    "\n",
    "response = r.get(url)\n",
    "\n",
    "#saving in the file\n",
    "html = response.text\n",
    "with open(\"index.html\",'w') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9243346f-9ed8-463f-9ddb-a112db1b8d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Scrap_time                                 Product_Name  \\\n",
      "0  2024-11-18 22:48:37.013160                      +3 MoreElite Men Gloves   \n",
      "1  2024-11-18 22:48:37.013160                     +4 MoreGorgeous Men Caps   \n",
      "2  2024-11-18 22:48:37.013160  01h : 26m : 23sFashionable Trendy Men Socks   \n",
      "3  2024-11-18 22:48:37.013160                       Fancy Trendy Men Socks   \n",
      "4  2024-11-18 22:48:37.013160                     Fashionate Men Bracelets   \n",
      "5  2024-11-18 22:48:37.013160               +3 MoreStyles Unique Men Socks   \n",
      "6  2024-11-18 22:48:37.013160                 Fashionable Modern Men Socks   \n",
      "7  2024-11-18 22:48:37.013160    +8 MoreFashionable Trendy Men Caps & Hats   \n",
      "8  2024-11-18 22:48:37.013160          +8 MoreFancy Latest Men Caps & Hats   \n",
      "9  2024-11-18 22:48:37.013160                      Casual Unique Men Socks   \n",
      "10 2024-11-18 22:48:37.013160                      Casual Unique Men Socks   \n",
      "11 2024-11-18 22:48:37.013160          +4 MoreFancy Modern Men Caps & Hats   \n",
      "12 2024-11-18 22:48:37.013160                      +4 MoreTrendy Bracelets   \n",
      "13 2024-11-18 22:48:37.013160                +8 MoreFancy Trendy Men Socks   \n",
      "14 2024-11-18 22:48:37.013160                     +4 MoreFabulous Men Caps   \n",
      "15 2024-11-18 22:48:37.013160                Fancy Latest Men Handkerchief   \n",
      "16 2024-11-18 22:48:37.013160                           Comfy Men Mufflers   \n",
      "17 2024-11-18 22:48:37.013160                Fancy Unique Men Handkerchief   \n",
      "18 2024-11-18 22:48:37.013160                +1 MoreFancy Unique Men Socks   \n",
      "19 2024-11-18 22:48:37.013160                      Casual Latest Men Socks   \n",
      "\n",
      "      Price          Rating  Reviews  \n",
      "0       218           3.223  Reviews  \n",
      "1       163        4.121473  Reviews  \n",
      "2   1451609         4.24103  Reviews  \n",
      "3   1911984         4.15852  Reviews  \n",
      "4       177           4.313  Reviews  \n",
      "5       258           3.715  Reviews  \n",
      "6       122         4.22787  Reviews  \n",
      "7       333             3.9        0  \n",
      "8       340             3.9        0  \n",
      "9       116           3.315  Reviews  \n",
      "10  1721762         4.41458  Reviews  \n",
      "11  1231359          3.8831  Reviews  \n",
      "12      166          3.8184  Reviews  \n",
      "13       58  ₹58₹703.815857  Reviews  \n",
      "14  1661713         4.08729  Reviews  \n",
      "15      245           4.013  Reviews  \n",
      "16      211         4.11862  Reviews  \n",
      "17      166           3.614  Reviews  \n",
      "18      136            1.84  Reviews  \n",
      "19      128         4.24504  Reviews  \n"
     ]
    }
   ],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def scrap_meescho(url):\n",
    "    # Make sure the URL is valid and not empty\n",
    "    if not url:\n",
    "        raise ValueError(\"URL cannot be empty\")\n",
    "    \n",
    "    # Fetch the webpage\n",
    "    response = r.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch URL. Status code: {response.status_code}\")\n",
    "    \n",
    "    html_data = response.content\n",
    "    soup = BeautifulSoup(html_data, 'lxml')\n",
    "    \n",
    "    # Define possible classes for product grids\n",
    "    all_class = [\n",
    "        'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA eCJiSA',\n",
    "        'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA jGAjKy',\n",
    "        'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA bGpzcx',\n",
    "        'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA ihwYlH'\n",
    "    ]\n",
    "    \n",
    "    # Initialize empty lists for scraped data\n",
    "    Rating = []\n",
    "    Products = []\n",
    "    Price = []\n",
    "    Reviews = []\n",
    "    \n",
    "    # Scrape data for each class\n",
    "    for cls in all_class:\n",
    "        for item in soup.find_all('div', {'class': cls}):\n",
    "            try:\n",
    "                prod, other_details = item.text.strip().split('₹', 1)\n",
    "                rest_d, rat_rev = other_details.split('Delivery')\n",
    "                pric = ''.join(filter(str.isdigit, rest_d.split()[0]))  # Extract price as digits only\n",
    "                \n",
    "                Products.append(prod.strip())\n",
    "                Price.append(pric.strip())\n",
    "                \n",
    "                if 'supplier' in rat_rev.lower():\n",
    "                    Rating.append(rat_rev[:3].strip())\n",
    "                    Reviews.append(0)\n",
    "                elif 'reviews' in rat_rev.lower():\n",
    "                    rating_data = rat_rev.split()\n",
    "                    Rating.append(rating_data[0].strip())\n",
    "                    Reviews.append(rating_data[1].strip() if len(rating_data) > 1 else 0)\n",
    "                else:\n",
    "                    Rating.append('N/A')\n",
    "                    Reviews.append('N/A')\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing item: {item.text.strip()}\\nError: {e}\")\n",
    "    \n",
    "    # Create a DataFrame with the scraped data\n",
    "    all_data = {\n",
    "        'Scrap_time': datetime.now(),\n",
    "        'Product_Name': Products,\n",
    "        'Price': Price,\n",
    "        'Rating': Rating,\n",
    "        'Reviews': Reviews\n",
    "    }\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "# Example Usage\n",
    "url = 'https://www.meesho.com/accessories-men/pl/3tp'\n",
    "scraped_df = scrap_meescho(url)\n",
    "print(scraped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d87e6b-3fd7-423d-92ef-a9eebe6fdd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extracted successfully\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Replace with your local file\n",
    "with open(\"mesooo_acce.html\", 'r') as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')\n",
    "\n",
    "all_class = [\n",
    "    'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA eCJiSA',\n",
    "    'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA jGAjKy',\n",
    "    'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA bGpzcx',\n",
    "    'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA ihwYlH'\n",
    "]\n",
    "\n",
    "Rating = []\n",
    "Products = []\n",
    "Price = []\n",
    "Reviews = []\n",
    "off_pri = []\n",
    "dis = []\n",
    "dilev_charg = []\n",
    "\n",
    "for cls in all_class:\n",
    "    for item in soup.find_all('div', {'class': cls}):\n",
    "        scrap_data = item.text.strip().split('₹')\n",
    "\n",
    "        if len(scrap_data) == 2:\n",
    "            prod, other_details = item.text.strip().split('₹')\n",
    "            rest_d, rat_rev = other_details.split(' Free Delivery')\n",
    "            pric = rest_d\n",
    "\n",
    "            Products.append(prod)\n",
    "            Price.append(pric)\n",
    "            off_pri.append(pric)\n",
    "            dis.append(0)\n",
    "            dilev_charg.append(0)\n",
    "            if 'supplier' in rat_rev.lower():\n",
    "                Rating.append(rat_rev[:3])\n",
    "                Reviews.append(0)\n",
    "            elif 'reviews' in rat_rev.lower():\n",
    "                rat_re, _ = rat_rev.split(' ')\n",
    "                Rating.append(rat_re[:3])\n",
    "                Reviews.append(rat_re[3:])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        elif len(scrap_data) == 3:\n",
    "            prod, offered_price, rest_details = scrap_data\n",
    "            Products.append(prod)\n",
    "            off_pri.append(offered_price)\n",
    "            pprice_dic, rate = rest_details.split(' offFree Delivery')\n",
    "            Price.append(pprice_dic[:3])\n",
    "            dis.append(pprice_dic[3:])\n",
    "            dilev_charg.append(0)\n",
    "            ret_rev = scrap_data[2].split(\" offFree Delivery\")[1]\n",
    "            rating = ret_rev.split(\" \")\n",
    "            Reviews.append(rating[0][3:])\n",
    "            Rating.append(rating[0][:3])\n",
    "\n",
    "        else:\n",
    "            Products.append(scrap_data[0])\n",
    "            off_pri.append(0)\n",
    "            Price.append(scrap_data[1].split(\" \")[0])\n",
    "            dilev_charg.append(scrap_data[2])\n",
    "            dis.append(0)\n",
    "            rat_revies = scrap_data[3].split(\" \")\n",
    "            Rating.append(rat_revies[0][2:5])\n",
    "            Reviews.append(rat_revies[0][5:])\n",
    "\n",
    "# Create a DataFrame with the extracted data\n",
    "all_data = {\n",
    "    'Scrap_time': datetime.now(),\n",
    "    'Product_Name': Products,\n",
    "    'Offered_Price': off_pri,\n",
    "    'Price': Price,\n",
    "    'Discount': dis,\n",
    "    'Rating': Rating,\n",
    "    'Reviews': Reviews,\n",
    "    'Delever_charge': dilev_charg\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"full_mesho.csv\", index=False)\n",
    "print(\"Data Extracted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27fe7a6e-c436-44c0-9ac6-3f5dd7f1166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working shell \n",
    "url='''https://www.meesho.com/home-decor/pl/3tl'''\n",
    "\n",
    "response = r.get(url)\n",
    "html_data = response.content\n",
    "soup = BeautifulSoup(html_data,'lxml')\n",
    "\n",
    "all_class = ['sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA eCJiSA','sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA jGAjKy',\n",
    "             'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA bGpzcx','sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA ihwYlH']\n",
    "\n",
    "\n",
    "Rating = []\n",
    "Products = []\n",
    "Price =[]\n",
    "Reviews =[]\n",
    "\n",
    "\n",
    "for cls in all_class:\n",
    "    for item in soup.find_all('div',{'class':cls}):\n",
    "        scrap_data = item.text.strip().split('₹')\n",
    "        \n",
    "      \n",
    "        prod, other_details = item.text.strip().split('₹')\n",
    "        rest_d,rat_rev=other_details.split('Delivery')\n",
    "       # print(other_details.split('Delivery'))\n",
    "        pric = rest_d[:3]   \n",
    "        Products.append(prod)\n",
    "        Price.append(pric)\n",
    "        if 'supplier' in rat_rev.lower():\n",
    "            Rating.append(rat_rev[:3])\n",
    "          #  print(rat_rev[:3])\n",
    "            Reviews.append(0)\n",
    "        elif 'reviews' in rat_rev.lower():            \n",
    "            Rating.append(rat_rev[:3])\n",
    "            Reviews.append(rat_rev[3:-7])\n",
    "            #print(rat_rev)\n",
    "        else:\n",
    "            Rating.append(0)\n",
    "            Reviews.append(0)\n",
    "       \n",
    "         \n",
    "                \n",
    "                    \n",
    "            \n",
    "                  \n",
    "\"\"\"print(len(Rating))\n",
    "print(len(Products))\n",
    "print(len(Price))\n",
    "print(len(Reviews)) \"\"\"\n",
    "\n",
    "all_data = {'Scrap_time':datetime.now(),'Product_Name':Products,'Price':Price,'Rating':Rating,'Reviews':Reviews}\n",
    "df = pd.DataFrame(all_data)\n",
    "df\n",
    "df.to_csv('mesho.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d19c8cec-a5e4-428f-8d1d-ac6602bb0272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extracted Successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scrap_time</th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Offered_Price</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Delever_charge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>Aakarsha Sensational Women Lehenga</td>\n",
       "      <td>505</td>\n",
       "      <td>554</td>\n",
       "      <td>9%</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+4 MoreBanita Graceful Women Lehenga</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5763</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+7 MoreMyra Fabulous Women Lehenga</td>\n",
       "      <td>552</td>\n",
       "      <td>575</td>\n",
       "      <td>4%</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>Kashvi Alluring Women Lehenga</td>\n",
       "      <td>754</td>\n",
       "      <td>754</td>\n",
       "      <td>0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1597</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+7 MoreJivika Sensational Women Lehenga</td>\n",
       "      <td>425</td>\n",
       "      <td>471</td>\n",
       "      <td>10%</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+3 MoreCharvi Sensational Women Lehenga</td>\n",
       "      <td>462</td>\n",
       "      <td>462</td>\n",
       "      <td>0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3379</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+8 MoreAakarsha Sensational Women Lehenga</td>\n",
       "      <td>288</td>\n",
       "      <td>300</td>\n",
       "      <td>4%</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>Alisha Refined Women Lehenga</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1406</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>Fancy Designer Flared Embroidered Indo Western...</td>\n",
       "      <td>646</td>\n",
       "      <td>646</td>\n",
       "      <td>0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>23336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+3 MoreFancy Designer Panelled Jacquard South ...</td>\n",
       "      <td>515</td>\n",
       "      <td>542</td>\n",
       "      <td>5%</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+3 MoreBanita Voguish Women Lehenga</td>\n",
       "      <td>727</td>\n",
       "      <td>748</td>\n",
       "      <td>3%</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+3 MoreAlisha Graceful Women Lehenga</td>\n",
       "      <td>446</td>\n",
       "      <td>446</td>\n",
       "      <td>0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+11 MoreFancy Designer Flared Embroidered Boll...</td>\n",
       "      <td>281</td>\n",
       "      <td>281</td>\n",
       "      <td>0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>11566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+3 MoreAagam Refined Women Lehenga</td>\n",
       "      <td>551</td>\n",
       "      <td>578</td>\n",
       "      <td>5%</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+4 MoreAagam Fabulous Women Lehenga</td>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+8 MoreAagyeyi Sensational Women Lehenga</td>\n",
       "      <td>958</td>\n",
       "      <td>100</td>\n",
       "      <td>75%</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+6 MoreFancy Designer Panelled Tassels And Lat...</td>\n",
       "      <td>610</td>\n",
       "      <td>641</td>\n",
       "      <td>5%</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2389</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+4 MoreFancy Designer Flared Embroidered Indo ...</td>\n",
       "      <td>292</td>\n",
       "      <td>292</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>+8 MoreCharvi Voguish Women Lehenga</td>\n",
       "      <td>530</td>\n",
       "      <td>551</td>\n",
       "      <td>4%</td>\n",
       "      <td>4.2</td>\n",
       "      <td>7414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2024-11-18 23:13:15.269721</td>\n",
       "      <td>00h : 01m : 45sTrendy Sensational Women Lehenga</td>\n",
       "      <td>432</td>\n",
       "      <td>459</td>\n",
       "      <td>6%</td>\n",
       "      <td>4.1</td>\n",
       "      <td>835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Scrap_time  \\\n",
       "0  2024-11-18 23:13:15.269721   \n",
       "1  2024-11-18 23:13:15.269721   \n",
       "2  2024-11-18 23:13:15.269721   \n",
       "3  2024-11-18 23:13:15.269721   \n",
       "4  2024-11-18 23:13:15.269721   \n",
       "5  2024-11-18 23:13:15.269721   \n",
       "6  2024-11-18 23:13:15.269721   \n",
       "7  2024-11-18 23:13:15.269721   \n",
       "8  2024-11-18 23:13:15.269721   \n",
       "9  2024-11-18 23:13:15.269721   \n",
       "10 2024-11-18 23:13:15.269721   \n",
       "11 2024-11-18 23:13:15.269721   \n",
       "12 2024-11-18 23:13:15.269721   \n",
       "13 2024-11-18 23:13:15.269721   \n",
       "14 2024-11-18 23:13:15.269721   \n",
       "15 2024-11-18 23:13:15.269721   \n",
       "16 2024-11-18 23:13:15.269721   \n",
       "17 2024-11-18 23:13:15.269721   \n",
       "18 2024-11-18 23:13:15.269721   \n",
       "19 2024-11-18 23:13:15.269721   \n",
       "\n",
       "                                         Product_Name Offered_Price Price  \\\n",
       "0                  Aakarsha Sensational Women Lehenga           505   554   \n",
       "1                +4 MoreBanita Graceful Women Lehenga           438   438   \n",
       "2                  +7 MoreMyra Fabulous Women Lehenga           552   575   \n",
       "3                       Kashvi Alluring Women Lehenga           754   754   \n",
       "4             +7 MoreJivika Sensational Women Lehenga           425   471   \n",
       "5             +3 MoreCharvi Sensational Women Lehenga           462   462   \n",
       "6           +8 MoreAakarsha Sensational Women Lehenga           288   300   \n",
       "7                        Alisha Refined Women Lehenga           490   490   \n",
       "8   Fancy Designer Flared Embroidered Indo Western...           646   646   \n",
       "9   +3 MoreFancy Designer Panelled Jacquard South ...           515   542   \n",
       "10                +3 MoreBanita Voguish Women Lehenga           727   748   \n",
       "11               +3 MoreAlisha Graceful Women Lehenga           446   446   \n",
       "12  +11 MoreFancy Designer Flared Embroidered Boll...           281   281   \n",
       "13                 +3 MoreAagam Refined Women Lehenga           551   578   \n",
       "14                +4 MoreAagam Fabulous Women Lehenga           548   548   \n",
       "15           +8 MoreAagyeyi Sensational Women Lehenga           958   100   \n",
       "16  +6 MoreFancy Designer Panelled Tassels And Lat...           610   641   \n",
       "17  +4 MoreFancy Designer Flared Embroidered Indo ...           292   292   \n",
       "18                +8 MoreCharvi Voguish Women Lehenga           530   551   \n",
       "19    00h : 01m : 45sTrendy Sensational Women Lehenga           432   459   \n",
       "\n",
       "   Discount Rating Reviews  Delever_charge  \n",
       "0        9%    4.0    1108               0  \n",
       "1         0    3.7    5763               0  \n",
       "2        4%    3.8    2452               0  \n",
       "3         0    3.9    1597               0  \n",
       "4       10%    4.1    1368               0  \n",
       "5         0    3.9    3379               0  \n",
       "6        4%    3.7    1464               0  \n",
       "7         0    3.9    1406               0  \n",
       "8         0    3.9   23336               0  \n",
       "9        5%    4.2    4806               0  \n",
       "10       3%    4.0    1970               0  \n",
       "11        0    3.9    1714               0  \n",
       "12        0    3.9   11566               0  \n",
       "13       5%    3.8    1008               0  \n",
       "14        0    4.0    1109               0  \n",
       "15      75%    3.9    2559               0  \n",
       "16       5%    3.9    2389               0  \n",
       "17        0    3.8    2279               0  \n",
       "18       4%    4.2    7414               0  \n",
       "19       6%    4.1     835               0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def soup_meesho(url):\n",
    "    # Fetch the webpage\n",
    "    response = r.get(url)\n",
    "    html_data = response.content\n",
    "    soup = BeautifulSoup(html_data, 'lxml')\n",
    "\n",
    "    # Classes to target product sections\n",
    "    all_class = [\n",
    "        'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA eCJiSA',\n",
    "        'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA jGAjKy',\n",
    "        'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA bGpzcx',\n",
    "        'sc-dkrFOg ProductList__GridCol-sc-8lnc8o-0 cokuZA ihwYlH'\n",
    "    ]\n",
    "\n",
    "    # Initialize data lists\n",
    "    Rating = []\n",
    "    Products = []\n",
    "    Price = []\n",
    "    Reviews = []\n",
    "    off_pri = []\n",
    "    dis = []\n",
    "    dilev_charg = []\n",
    "\n",
    "    for cls in all_class:\n",
    "        for item in soup.find_all('div', {'class': cls}):\n",
    "            scrap_data = item.text.strip().split('₹')\n",
    "\n",
    "            if len(scrap_data) == 2:\n",
    "                prod, other_details = scrap_data\n",
    "                rest_d, rat_rev = other_details.split(' Free Delivery')\n",
    "                pric = rest_d\n",
    "\n",
    "                Products.append(prod)\n",
    "                Price.append(pric)\n",
    "                off_pri.append(pric)\n",
    "                dis.append(0)\n",
    "                dilev_charg.append(0)\n",
    "\n",
    "                if 'supplier' in rat_rev.lower():\n",
    "                    Rating.append(rat_rev[:3])\n",
    "                    Reviews.append(0)\n",
    "                elif 'reviews' in rat_rev.lower():\n",
    "                    rat_re, _ = rat_rev.split(' ')\n",
    "                    Rating.append(rat_re[:3])\n",
    "                    Reviews.append(rat_re[3:])\n",
    "                else:\n",
    "                    Rating.append(0)\n",
    "                    Reviews.append(0)\n",
    "\n",
    "            elif len(scrap_data) == 3:\n",
    "                prod, offered_price, rest_details = scrap_data\n",
    "                Products.append(prod)\n",
    "                off_pri.append(offered_price)\n",
    "                pprice_dic, rate = rest_details.split(' offFree Delivery')\n",
    "                Price.append(pprice_dic[:3])\n",
    "                dis.append(pprice_dic[3:])\n",
    "                dilev_charg.append(0)\n",
    "\n",
    "                ret_rev = rest_details.split(\" offFree Delivery\")[1]\n",
    "                rating = ret_rev.split(\" \")\n",
    "                Rating.append(rating[0][:3])\n",
    "                Reviews.append(rating[0][3:])\n",
    "            else:\n",
    "                Products.append(scrap_data[0])\n",
    "                off_pri.append(0)\n",
    "                Price.append(scrap_data[1].split(\" \")[0])\n",
    "                dilev_charg.append(scrap_data[2])\n",
    "                dis.append(0)\n",
    "                rat_revies = scrap_data[3].split(\" \")\n",
    "                Rating.append(rat_revies[0][2:5])\n",
    "                Reviews.append(rat_revies[0][5:])\n",
    "\n",
    "    # Define filename from URL\n",
    "    file_name = url.split(\"https://www.meesho.com/\")[1].split(\"/\")[0]\n",
    "\n",
    "    # Prepare data dictionary\n",
    "    all_data = {'Scrap_time': datetime.now(), 'Product_Name': Products, 'Offered_Price': off_pri, 'Price': Price,\n",
    "                'Discount': dis, 'Rating': Rating, 'Reviews': Reviews, 'Delever_charge': dilev_charg}\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    table_new = pd.DataFrame(all_data)\n",
    "\n",
    "    # Define folder and check if file exists\n",
    "    folder_path = '/Users/nsherpa/iCloud Drive (Archive) - 1/Documents'\n",
    "    folder_finders = os.listdir(folder_path)\n",
    "\n",
    "    # Check if file exists and append data if it does\n",
    "    if f'{file_name}.csv' in folder_finders:\n",
    "        table_old = pd.read_csv(f\"{folder_path}/{file_name}.csv\")\n",
    "        final_table = pd.concat([table_old, table_new])\n",
    "        final_table.to_csv(f'{folder_path}/{file_name}.csv', index=False)\n",
    "        print(\"Data Appended to the existing Table\")\n",
    "    else:\n",
    "        # Save new data if file does not exist\n",
    "        table_new.to_csv(f'{folder_path}/{file_name}.csv', index=False)\n",
    "        print(\"Data Extracted Successfully\")\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return table_new\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.meesho.com/lehengas/pl/3l6\"\n",
    "soup_meesho(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f06fe-1e78-4f58-a670-8d7e15df2ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
